#开题报告（无格式版）
by Hence@Lancet

###选题的背景和意义

**爬虫技术的复杂性与多样性**
在以数据为中心的信息时代，如何快速廉价地获取大量有效的数据，是维持高度数据依赖的组织的竞争力的关键。而在真实互联网环境，数据往往是不均匀地散落在以web为代表的互联网的各个角落中。为了更快更廉价的获取数据，很多互联网组织和个人开发了各式各样的网络爬虫（crawler）来抓取他们感兴趣的数据。爬虫在互联网数据传播过程中起到了至关重要的作用。

现有爬虫种类多，爬取的策略也具有很高的复杂性和多样性。爬虫一般可以按其特点分为四类：通用网络爬虫，聚焦网络爬虫，增量式网络爬虫，深层网络爬虫。最常见的爬取策略是广度优先策略和深度优先策略。除此此外，比较常见的聚焦网络爬虫，甚至会对访问页面和链接的重要性进行评分，并以此作为页面爬取先后的顺序的参考依据【引用】。

Paul在他的论文中提出了Fish-Search算法【Information Retrieval in Distributed Hypertexts】，以页面内容与爬虫的聚焦主题的相关性作为度量标准。Page rank算法主要用于对搜索引擎搜索到的内容进行结果排序，也可以用于评价链接的重要性。Rennie 和 McCallum在他们的工作中，将增强学习（reinforcement learning)引入聚焦爬虫，他们试图通过机器学习找到一组策略，使得对于该策略的激励达到最优解。Diligenti则提出一种通过建立语境图来描述不同网页之间相关度的算法，以此作为确定访问顺序的参考。

爬虫技术的复杂性和多样性也意味着反爬虫技术的复杂性。

**恶意爬虫带来的威胁**
网络中同样充斥着大量恶意的爬虫，他们要么多线程高并发地请求服务器，要么抓取一些敏感的或者高价值的数据，转手将数据倒卖给商业竞争对手。因此，针对爬虫反制措施的重要性也凸显了出来。据资料显示，每年的三月份，正处在在大量硕士需要撰写论文的时间节点，网络着大量应届毕业生写的劣质爬虫，导致很多数据业务相关的网站在三月份访问量大幅度增加。有一部分失控爬虫，不断地像相关网站发送大量请求，造成的带宽的严重占用和流量资源的损耗。今年十月份爆出的马蜂窝评论数据涉嫌造假事件，其数据来源就是通过爬虫，从其竞争对手的网站上抓取的。在涉嫌欺诈的同时，也给其竞争对手带来大量的经济损失。
此外，一部分黑客在攻击网站之前，会通过通用爬虫爬取网站的所有可访问资源，并尝试对其中的参数进行模糊测试。尽早发现带有恶意payload攻击的爬虫，能够在真实攻击发生的早期，进行有效的监测和加固。




**爬虫技术的更新与迭代**
随着headless browser工具的兴起，以及以selenium为主的自动化web测试工具的出现，使得爬虫技术也从原始的静态网页处理方式，逐渐进化到能够与动态的javascript进行复杂交互的状态。这也使得大部分基于javascript脚本混淆（javascipt obfuscate）的反爬虫技术失去了原有的优势。此外，大部分学术界提出的【加引用】爬虫检测策略，往往基于流量日志，是非实时的（offline）检测机制，即不能对爬虫行为做出及时的反馈，也会在随后的离线分析中损耗大量的人力成本。

网络中充斥着大量的网络代理节点，大量的爬虫会在爬取网站过程中不断更改访问的IP地址，使得原本针对IP封锁的方案往往达不到预期的效果，一旦误封正常用户的IP地址，可能造成用户群体的流失。即使是基于CAPTCHA验证码的访问验证，也可以通过打码平台的技术支持在一定程度上进行绕过，而且会影响到正常用户的访问体验。

随着爬虫技术的更新与迭代，甚至是反反爬虫技术的逐渐成熟，现有的反爬虫检测技术已经不能满足当下严峻形势的需求，而爬虫处理技术过于单一，使得网站维护者们在反爬虫的战争中占尽劣势。


**爬虫攻击的可行性**
大部分的爬虫都是使用python进行编写的，其常用的数据存储方式一般采用传统数据库加文件持久化的方案，在加载网页的过程中，还可能使用headless browser对爬取到的javascript进行解析。因此，基于以上的爬虫架构，针对于python语言特性，数据库乃至于javascript解析的传统攻击手段也有可能可以利用成功。基于这样的一种直觉性的判断，我们可以针对识别出爬虫的具体特征，比如说是否使用python编写，是否开启了javascript，大致使用了什么类型的数据库（可以通过javascript进行扫描），这些特性信息均可以通过与爬虫之间的简单交互获得，进一步地，生成相应的攻击payload来尝试攻击爬虫系统以使之奔溃甚至获得爬虫宿主机的权限。
除了针对传统架构传统攻击方式之外，我通过观察发现，爬虫系统使用的headless browser，phantomjs占有很大的市场份额。而且，在phantomjs的github主页上，我们可以发现接近2000条issue，而且其中很多issue展示出的问题，都是关于phantomjs在使用某些处理逻辑处理相关网页时，整个程序出现未知错误而退出，这些问题大部分没有被复现和修复。
因此，深入研究这些因为底层webdriver问题带来的收益也是巨大的。我们也许能够发现大量webdriver二进制程序的未知漏洞，这些漏洞能够时爬虫进程奔溃，但又不影响到正常用户的访问，我们甚至可以利用这些漏洞获取爬虫宿主机的权限。


**反爬虫系统的意义**
互联网时代，数据的价值无需置疑。而爬虫技术与反反爬虫技术不断更新迭代，相应的反爬虫技术却一直停滞不前。很多优秀的爬虫算法要么不具有实时性，要么需要大量的训练数据，要么还是停留在概念阶段，难以在实战中发挥作用。而传统反爬虫技术中爬虫处理和阻断部分过于单一与被动，不能对爬虫施加有效制裁和限制。因此，一个实时性，多样化，主动的反爬虫系统的实现将具有重大的意义，它能够扭转当前爬虫与反爬虫技术不对等的状况。
本文不仅包含整个爬虫系统的设计和实现，还将提供相应的部署方案，以及对爬虫系统部署之后爬虫的识别正确率，识别覆盖率，性能损失作出相应的评估。



###相关技术研究现状
**爬虫识别技术研究现状**
目前主流的爬虫识别技术可以分为四大类：
日志语法分析模式（Syntactical log analysis），流量分析模式（Traffic pattern analysis），分析学习（Analytical learning techniques）以及图灵测试系统（Turing test systems）。

* 日志语法分析模式：
Holloway以及keller提出了独立字段解析（individual field parsing）的方法。他们通过比较HTTP请求中独立字段与数据库中的爬虫常使用关键字和banner信息，来判定请求是否来源于爬虫。但是，一部分未知爬虫的UA特征可能并未收集在数据库中还有一些爬虫会修饰其的User-Agent字段，使得通过UA进行单一爬虫检测的漏检率很高。
Huntington等人则采用了多层次日志解析（multifaceted log analysis）的策略：他们首先将请求robots.txt的访问者被标记为爬虫，这样的流量大概占总流量比例的0.5%；然后，再对日志中的访问IP地址做反向DNS查询，如果dns解析得到的域名中包含诸如robot, bot, search, spider以及crawl这样的关键字，则将访问者标记为爬虫，使用该方法大概能检测到16.1%的流量来源于爬虫；最后，通过从网络搜集的爬虫池的IP地址段，来标记来源于爬虫IP的访问。

* 流量分析模式
Geens等人最早提出了将语法分析与流量分析模式结合在一起的检测方法，并给定了两个评估参数:
r 覆盖率 r = 检测出爬虫数量/实际爬虫数量
p 准确率 p = 检测出爬虫数量/被检测器标记为爬虫的数量
通过人工标记数据的方法，分别计算每种分析模式的r和p，并最终得出了相应的检测策略。
guo等人利用聚焦网络爬虫特性，提出了一种基于资源请求模式分析（resource request patterns）的爬虫检测方法。他们将请求的url资源分为网页，文档，脚本，图片，视频，下载等多种类型。然后他们将依据IP，UA这些信息，区分不同的请求客户端，并统计每一个客户端针对每种资源的访问次数。如果访问次数过多或者访问占比过多，则将访问者判定为爬虫。
在Duskin的文章中，他通过分析请求速率模式（query rate patterns）来区分正常访问和爬虫。首先统计请求提交速率，请求平均时间间隔，session的活动持续时间，请求时间与作息时间的相关性。然后再根据分析这些特性得到的结果来区分用户和爬虫。

* 分析学习
Tan and Kumar 利用爬虫的session中的一些特征向量来区分人类用户与爬虫。首先，他们从日志数据中，将所有的请求按照session进行分类，并选择特征向量以及对数据进行标记。以该标记数据为学习数据使用 C4.5 决策树算法构造决策树（decision tree）。
Lu and Yu 在2006年提出一个基于请求到达模式的用户与爬虫识别程序。他们把人类访问的特征概括为访问所有内嵌页面资源的突发性访问HTTP请求，在整个session过程中，会因为阅读和处理页面内容而存在不活跃时间。而爬虫则会以一个稳定均匀的速率爬行特定类型的资源。在他们提出的算法中，他们先将流量按等时间段切片，在每个时间片中会有一到多个请求到达。通过UA来判定是否为爬虫，并以此来标记数据训练HMM。在最后，以未来的请求序列作为已训练的HMM模型的输入，使用forward-backward procedure来计算其可能是爬虫请求的概率。
Completely Automated Public Turing test to tell Computers and Humans Apart（CAPTCHA）最早在2003年由Ahn等人提出。CAPTCHA通常都是以验证码形式存在，设计的目标是让真实使用者更容易通过测试而爬虫更难通过测试。


**浏览指纹技术研究现状**



**二进制漏洞挖掘技术现状**



###论文的研究内容及拟采用的技术方案
反爬虫系统设计图
实时识别与序列识别解决方案
爬虫溯源技术
爬虫漏洞挖掘与利用
系统部署方案
系统评估方案

###关键技术
动态符号执行与模糊测试





###论文研究计划


###主要参考文献